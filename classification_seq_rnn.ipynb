{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pdb\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "\n",
    "\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# cudnn.benchmark = True\n",
    "import os\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "\n",
    "GPU_ids = [1]\n",
    "GPU_id = GPU_ids[0]\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(ID) for ID in GPU_ids])\n",
    "GPU_ids = list(range(0, len(GPU_ids)))\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "import torch\n",
    "import torch.utils.data \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from seq2loc.utils import *\n",
    "\n",
    "import math\n",
    "\n",
    "class SequenceDatasetDummy(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequence_path = './data/uniprot.csv', max_seq_len = 25, mlb = None):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.dummy_seq = ['', 'ASDFASDFASDF', 'FAAAAAAAAD']\n",
    "        \n",
    "        column_name = 'GO id'\n",
    "        \n",
    "        df = pd.read_csv(sequence_path)\n",
    "\n",
    "        df = df.dropna(subset=[column_name])\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        if mlb is None:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit([self.dummy_seq])\n",
    "        \n",
    "\n",
    "        some_hot_targets = mlb.transform([self.dummy_seq])\n",
    "    \n",
    "        df_sequences = df['Sequence']\n",
    "\n",
    "        #Trim out the bonkers long sequences\n",
    "        seq_lengths = [len(seq) for seq in df_sequences]\n",
    "        max_len = np.percentile(seq_lengths, 99.5)\n",
    "        \n",
    "        keep_inds = seq_lengths <= max_len\n",
    "        \n",
    "        df_sequences = df_sequences[keep_inds].reset_index(drop=True)\n",
    "\n",
    "        self.somehot = some_hot_targets\n",
    "        self.pd_sequences = df_sequences\n",
    "        \n",
    "        self.mlb = mlb\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        class_index = np.random.randint(len(self.dummy_seq))\n",
    "        \n",
    "        seq = self.pd_sequences[index]\n",
    "        \n",
    "        if len(seq) > self.max_seq_len:\n",
    "            start = np.random.randint(len(seq)-self.max_seq_len)\n",
    "            seq = seq[start:(start+self.max_seq_len)]\n",
    "            \n",
    "        sequence_to_insert = self.dummy_seq[class_index]\n",
    "        splitloc = np.random.randint(len(seq))\n",
    "        \n",
    "        seq = seq[0:splitloc] + sequence_to_insert + seq[splitloc:]\n",
    "            \n",
    "        tensor_indices = lineToIndices(seq)\n",
    "\n",
    "            \n",
    "        label = torch.zeros(len(self.dummy_seq))\n",
    "        label[class_index] = 1\n",
    "            \n",
    "        return tensor_indices, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pd_sequences)\n",
    "    \n",
    "\n",
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequence_path = './data/uniprot.csv', max_seq_len = 25, mlb = None):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        column_name = 'GO id'\n",
    "        \n",
    "        df = pd.read_csv(sequence_path)\n",
    "\n",
    "        df = df.dropna(subset=[column_name])\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        if mlb is None:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit(df[column_name].str.split(';').tolist())\n",
    "        \n",
    "        some_hot_targets = mlb.transform(df[column_name].str.split(';').tolist())\n",
    "        df['GOsomehot'] = pd.Series(tuple(some_hot_targets.astype(np.float32)))\n",
    "\n",
    "        \n",
    "        df_somehot = df['GOsomehot']\n",
    "        \n",
    "        df_sequences = df['Sequence']\n",
    "\n",
    "        #Trim out the bonkers long sequences\n",
    "        seq_lengths = [len(seq) for seq in df_sequences]\n",
    "        max_len = np.percentile(seq_lengths, 99.5)\n",
    "        \n",
    "        keep_inds = seq_lengths <= max_len\n",
    "        \n",
    "        df_sequences = df_sequences[keep_inds].reset_index(drop=True)\n",
    "\n",
    "        self.somehot = df_somehot[keep_inds].reset_index(drop=True)\n",
    "        self.pd_sequences = df_sequences\n",
    "        \n",
    "        self.mlb = mlb\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        seq = self.pd_sequences[index]\n",
    "        \n",
    "        if len(seq) <= self.max_seq_len:\n",
    "            tensor_indices = lineToIndices(seq)\n",
    "        else:\n",
    "            start = np.random.randint(len(seq)-self.max_seq_len)\n",
    "            \n",
    "            tensor_indices = lineToIndices(seq[start:(start+self.max_seq_len)])\n",
    "            \n",
    "        somehot = self.somehot[index]\n",
    "            \n",
    "        return tensor_indices, torch.Tensor(somehot)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pd_sequences)\n",
    "    \n",
    "class PaddedSequenceDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, sequenceDataset, GPU_id = None):\n",
    "            self.sequenceDataset = sequenceDataset\n",
    "            self.GPU_id = GPU_id\n",
    "            \n",
    "        def __getitem__(self, indices):\n",
    "            \n",
    "            sequence_tensor_indices = list()\n",
    "            somehots = list()\n",
    "            \n",
    "            #get all the sequences as a list of character indices\n",
    "            for index in indices:\n",
    "                tensor_indices, somehot = self.sequenceDataset[index]\n",
    "                sequence_tensor_indices += [Variable(tensor_indices)]\n",
    "                somehots += [Variable(somehot)]\n",
    "                \n",
    "            #get the longest sequence\n",
    "            ind = np.argmax([len(s) for s in sequence_tensor_indices])\n",
    "            \n",
    "            tensor_len = len(sequence_tensor_indices[ind])\n",
    "            nchars = n_letters()\n",
    "            \n",
    "            #pad all shorter sequences with the stop character\n",
    "            for i in range(len(sequence_tensor_indices)):\n",
    "                \n",
    "                my_inds = sequence_tensor_indices[i]\n",
    "                my_len = my_inds.shape[0]\n",
    "\n",
    "                sequence_tensor_indices[i] = torch.unsqueeze(torch.cat([my_inds, Variable(torch.ones(tensor_len - my_len).long()*(nchars-1))]), 1)\n",
    "            \n",
    "            sequence_tensor_indices = torch.cat(sequence_tensor_indices, 1)\n",
    "            \n",
    "            somehots = torch.stack(somehots)\n",
    "            \n",
    "            sequence_tensors = Variable(indicesToTensor(sequence_tensor_indices))\n",
    "            \n",
    "            if self.GPU_id is not None:\n",
    "                sequence_tensors = sequence_tensors.cuda(self.GPU_id)\n",
    "                somehots = somehots.cuda(self.GPU_id)\n",
    "                \n",
    "            return sequence_tensors, somehots            \n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.sequenceDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, attn_in_size, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "                                    nn.Linear(attn_in_size, hidden_size)\n",
    "                                )\n",
    "        \n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs_tmp = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs_tmp)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "        \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        \n",
    "        return context, attn_weights\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        energy = self.attn(torch.cat([hidden, encoder_outputs], 2))\n",
    "        \n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]\n",
    "\n",
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_gru_layers=1, bidirectional = True, dropout_p = 0.2):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "        \n",
    "        self.h_layers = num_gru_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_gru_layers = num_gru_layers\n",
    "        self.h_layers = num_gru_layers\n",
    "        \n",
    "        n_attention_heads = 6\n",
    "        \n",
    "        lin_in = self.h_layers * self.hidden_size\n",
    "        \n",
    "        gru_out_size = hidden_size\n",
    "        if bidirectional:\n",
    "            self.h_layers *= 2\n",
    "            lin_in *=  2\n",
    "            gru_out_size *= 2\n",
    "    \n",
    "            \n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers = num_gru_layers, bidirectional = bidirectional, dropout = dropout_p)\n",
    "        \n",
    "        \n",
    "        self.atten_list = nn.ModuleList(Attention(lin_in+ gru_out_size, hidden_size) for i in range(n_attention_heads))\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "                                 nn.Linear(lin_in + gru_out_size*n_attention_heads, output_size)\n",
    "                                )\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        output, hidden = self.gru(input, hidden)     \n",
    "        \n",
    "        hidden = torch.cat([h for h in hidden],1)\n",
    "        \n",
    "        context_list, attn_list = list(), list()\n",
    "        for i, atten_head in enumerate(self.atten_list):\n",
    "            \n",
    "            context, attn_weights = atten_head(hidden, output)\n",
    "            \n",
    "            context_list.append(torch.squeeze(context.transpose(0, 1)))\n",
    "            attn_list.append(attn_weights)\n",
    "\n",
    "        output = torch.cat([hidden] + context_list, 1)\n",
    "\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def initHidden(self, batch_size = 1):\n",
    "        result = Variable(torch.zeros(self.h_layers, batch_size, self.hidden_size))\n",
    "        \n",
    "        return result\n",
    "#         if use_cuda:\n",
    "#             return result.cuda()\n",
    "#         else:\n",
    "#             return result\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52789aea24bc4ee48acee9fd1f0c1ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1863), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from decimal import Decimal\n",
    "\n",
    "import seq2loc.utils as utils\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# from seq2loc.data.datasets import PaddedSequenceDataset, SequenceDataset\n",
    "\n",
    "\n",
    "GPU_id = 0\n",
    "LR = 0.001\n",
    "N_EPOCHS = 500\n",
    "hidden_size = 256\n",
    "batch_size = 32\n",
    "num_gru_layers = 2\n",
    "\n",
    "N_LETTERS = utils.n_letters()\n",
    "\n",
    "grad_clip = 10.0\n",
    "\n",
    "bidirectional=True\n",
    "\n",
    "#really big number to trim sequences to\n",
    "max_seq_len = 4000\n",
    "\n",
    "ds = PaddedSequenceDataset(SequenceDataset('./data/hpa_data_resized_train.csv', max_seq_len = max_seq_len), GPU_id = GPU_id)\n",
    "ds_test = PaddedSequenceDataset(SequenceDataset('./data/hpa_data_resized_test.csv', max_seq_len = max_seq_len, mlb = ds.sequenceDataset.mlb), GPU_id = GPU_id)\n",
    "ds_validate = PaddedSequenceDataset(SequenceDataset('./data/hpa_data_resized_validate.csv', max_seq_len = max_seq_len, mlb = ds.sequenceDataset.mlb), GPU_id = GPU_id)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "enc = SequenceClassifier(N_LETTERS, 33, hidden_size, num_gru_layers, bidirectional=bidirectional).cuda(GPU_id)\n",
    "\n",
    "opt = optim.Adam(enc.parameters(), lr = LR)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "save_dir = './classifier/{}'\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    epoch_inds = utils.get_epoch_inds(len(ds), batch_size)\n",
    "    pbar = tqdm(epoch_inds)\n",
    "\n",
    "    epoch_losses = list()\n",
    "    \n",
    "    for batch in pbar:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        x, y = ds[batch]        \n",
    "        \n",
    "        hidden = enc.initHidden(batch_size).cuda(GPU_id)\n",
    "        y_hat  = enc(x, hidden)\n",
    "        \n",
    "        loss = criterion(y_hat, y)\n",
    "     \n",
    "        loss.backward()\n",
    "        clip_grad_norm(enc.parameters(), grad_clip)\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        losses_np = np.squeeze(loss.detach().cpu().numpy())\n",
    "        \n",
    "        epoch_losses += [losses_np]\n",
    "    #     t.set_description('GEN %i' % i)\n",
    "        pbar.set_description('%.4E' % Decimal(str(losses_np)))\n",
    "        \n",
    "        writer.add_scalars(save_dir.format('train'), {'epoch': epoch,\n",
    "                                                        'loss': losses_np}, iteration)\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "\n",
    "    enc.train(False)\n",
    "    \n",
    "    x, y = ds_test[utils.get_epoch_inds(len(ds_test), batch_size)[0]]\n",
    "    hidden = enc.initHidden(batch_size).cuda(GPU_id)\n",
    "    with torch.no_grad():\n",
    "        y_hat  = enc(x, hidden)\n",
    "    loss = criterion(y_hat, y)\n",
    "    losses_test = np.squeeze(loss.detach().cpu().numpy())\n",
    "    \n",
    "    writer.add_scalars(save_dir.format('test'), {'epoch': epoch,\n",
    "                                                 'loss': losses_np}, iteration)\n",
    "    \n",
    "    enc.train(True)\n",
    "    \n",
    "    \n",
    "    pbar.set_description('%.4E' % Decimal(str(np.mean(epoch_losses))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/sklearn/metrics/ranking.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifier(\n",
       "  (gru): GRU(23, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "  (atten_list): ModuleList(\n",
       "    (0): Attention(\n",
       "      (attn): Sequential(\n",
       "        (0): Linear(in_features=1536, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Attention(\n",
       "      (attn): Sequential(\n",
       "        (0): Linear(in_features=1536, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Attention(\n",
       "      (attn): Sequential(\n",
       "        (0): Linear(in_features=1536, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Attention(\n",
       "      (attn): Sequential(\n",
       "        (0): Linear(in_features=1536, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Attention(\n",
       "      (attn): Sequential(\n",
       "        (0): Linear(in_features=1536, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Attention(\n",
       "      (attn): Sequential(\n",
       "        (0): Linear(in_features=1536, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=33, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
    "mlb = ds.sequenceDataset.mlb\n",
    "# enc.train(False)\n",
    "\n",
    "# epoch_inds = utils.get_epoch_inds(len(ds_test), batch_size)\n",
    "\n",
    "# y_list = list()\n",
    "# y_hat_list = list()\n",
    "\n",
    "# for batch in tqdm(epoch_inds):\n",
    "    \n",
    "#     x, y = ds_test[batch]        \n",
    "#     hidden = enc.initHidden(batch_size).cuda(GPU_id)\n",
    "#     with torch.no_grad():\n",
    "#         y_hat  = nn.Sigmoid()(enc(x, hidden))\n",
    "    \n",
    "    \n",
    "#     y_list += [y]\n",
    "#     y_hat_list += [y_hat]\n",
    "    \n",
    "\n",
    "# y = torch.cat(y_list).cpu().numpy()\n",
    "# y_hat = torch.cat(y_hat_list).cpu().numpy()\n",
    "\n",
    "# thresh = 0.5\n",
    "\n",
    "\n",
    "true_labs = y\n",
    "pred_acts = y_hat\n",
    "pred_labs = np.zeros_like(pred_acts)\n",
    "pred_labs[pred_acts > thresh] = 1\n",
    "\n",
    "df_stats = pd.DataFrame()\n",
    "for i,col in enumerate(mlb.classes_):\n",
    "\n",
    "    # get true labels and predicted activations\n",
    "    true_labs_col = true_labs[:,i]\n",
    "    pred_acts_col = pred_acts[:,i]\n",
    "    pred_labs_col = pred_labs[:,i]\n",
    "\n",
    "    # compute one against all prec + recall stats\n",
    "    p,r,f,_ = precision_recall_fscore_support(true_labs_col,pred_labs_col, average='binary')\n",
    "    df_stats.loc[i,'precision_{}'.format(thresh)] = p\n",
    "    df_stats.loc[i,'recall_{}'.format(thresh)] = r\n",
    "    df_stats.loc[i,'f1score_{}'.format(thresh)] = f\n",
    "    df_stats.loc[i,'auprc'] = average_precision_score(true_labs_col, pred_acts_col)\n",
    "    df_stats.loc[i,'support'] = int(true_labs_col.sum())\n",
    "    df_stats.loc[i,'label'] = col\n",
    "    \n",
    "    \n",
    "    \n",
    "enc.train(True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_0.5</th>\n",
       "      <th>recall_0.5</th>\n",
       "      <th>f1score_0.5</th>\n",
       "      <th>auprc</th>\n",
       "      <th>support</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Actin filaments (GO:0015629)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015530</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Aggresome (GO:0016235)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.134021</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.117678</td>\n",
       "      <td>194.0</td>\n",
       "      <td>Cell Junctions (GO:0030054)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>169.0</td>\n",
       "      <td>Centrosome (GO:0005813)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cleavage furrow (GO:0032154)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Cytokinetic bridge (GO:0045171)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Cytoplasmic bodies (GO:0036464)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.443589</td>\n",
       "      <td>0.499811</td>\n",
       "      <td>0.470025</td>\n",
       "      <td>0.445268</td>\n",
       "      <td>2651.0</td>\n",
       "      <td>Cytosol (GO:0005829)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.110599</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.226962</td>\n",
       "      <td>217.0</td>\n",
       "      <td>Endoplasmic reticulum (GO:0005783)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Endosomes (GO:0005768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008770</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Focal adhesion sites (GO:0005925)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.177419</td>\n",
       "      <td>0.048458</td>\n",
       "      <td>0.076125</td>\n",
       "      <td>0.136521</td>\n",
       "      <td>681.0</td>\n",
       "      <td>Golgi apparatus (GO:0005794)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.085756</td>\n",
       "      <td>146.0</td>\n",
       "      <td>Intermediate filaments (GO:0045111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Lipid droplets (GO:0005811)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Lysosomes (GO:0005764)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Microtubule ends (GO:1990752)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039443</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Microtubule organizing center (GO:0005815)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.059863</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Microtubules (GO:0015630)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Midbody (GO:0030496)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Midbody ring (GO:0090543)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.394009</td>\n",
       "      <td>0.279869</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.312390</td>\n",
       "      <td>611.0</td>\n",
       "      <td>Mitochondria (GO:0005739)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Mitotic spindle (GO:0072686)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>0.067246</td>\n",
       "      <td>329.0</td>\n",
       "      <td>Nuclear bodies (GO:0016604)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.067346</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Nuclear membrane (GO:0031965)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.037039</td>\n",
       "      <td>258.0</td>\n",
       "      <td>Nuclear speckles (GO:0016607)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.133803</td>\n",
       "      <td>0.212291</td>\n",
       "      <td>0.229091</td>\n",
       "      <td>852.0</td>\n",
       "      <td>Nucleoli (GO:0005730)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.037247</td>\n",
       "      <td>154.0</td>\n",
       "      <td>Nucleoli fibrillar center (GO:0001650)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.406179</td>\n",
       "      <td>0.340701</td>\n",
       "      <td>0.370570</td>\n",
       "      <td>0.420419</td>\n",
       "      <td>2624.0</td>\n",
       "      <td>Nucleoplasm (GO:0005654)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.345815</td>\n",
       "      <td>0.122656</td>\n",
       "      <td>0.181084</td>\n",
       "      <td>0.252189</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>Nucleus (GO:0005634)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Peroxisomes (GO:0005777)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.227806</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.190210</td>\n",
       "      <td>0.226293</td>\n",
       "      <td>833.0</td>\n",
       "      <td>Plasma membrane (GO:0005886)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Rods &amp; Rings ()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.408108</td>\n",
       "      <td>0.125624</td>\n",
       "      <td>0.192112</td>\n",
       "      <td>0.272481</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>Vesicles (GO:0043231)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    precision_0.5  recall_0.5  f1score_0.5     auprc  support  \\\n",
       "0        0.000000    0.000000     0.000000  0.029031     88.0   \n",
       "1        0.000000    0.000000     0.000000  0.015530     18.0   \n",
       "2        0.393939    0.134021     0.200000  0.117678    194.0   \n",
       "3        0.000000    0.000000     0.000000  0.017745    169.0   \n",
       "4        0.000000    0.000000     0.000000       NaN      0.0   \n",
       "5        0.000000    0.000000     0.000000  0.006784     71.0   \n",
       "6        0.000000    0.000000     0.000000  0.002286     28.0   \n",
       "7        0.443589    0.499811     0.470025  0.445268   2651.0   \n",
       "8        0.369231    0.110599     0.170213  0.226962    217.0   \n",
       "9        0.000000    0.000000     0.000000       NaN      0.0   \n",
       "10       0.000000    0.000000     0.000000  0.008770     66.0   \n",
       "11       0.177419    0.048458     0.076125  0.136521    681.0   \n",
       "12       1.000000    0.041096     0.078947  0.085756    146.0   \n",
       "13       0.000000    0.000000     0.000000  0.002898     24.0   \n",
       "14       0.000000    0.000000     0.000000  0.000616      8.0   \n",
       "15       0.000000    0.000000     0.000000  0.000676      6.0   \n",
       "16       0.000000    0.000000     0.000000  0.039443     65.0   \n",
       "17       0.428571    0.030612     0.057143  0.059863    196.0   \n",
       "18       0.000000    0.000000     0.000000  0.006311     26.0   \n",
       "19       0.000000    0.000000     0.000000  0.001363     12.0   \n",
       "20       0.394009    0.279869     0.327273  0.312390    611.0   \n",
       "21       0.000000    0.000000     0.000000  0.002222     10.0   \n",
       "22       0.130435    0.036474     0.057007  0.067246    329.0   \n",
       "23       0.333333    0.033333     0.060606  0.067346    180.0   \n",
       "24       0.142857    0.023256     0.040000  0.037039    258.0   \n",
       "25       0.513514    0.133803     0.212291  0.229091    852.0   \n",
       "26       0.040000    0.012987     0.019608  0.037247    154.0   \n",
       "27       0.406179    0.340701     0.370570  0.420419   2624.0   \n",
       "28       0.345815    0.122656     0.181084  0.252189   1280.0   \n",
       "29       0.000000    0.000000     0.000000       NaN      0.0   \n",
       "30       0.227806    0.163265     0.190210  0.226293    833.0   \n",
       "31       0.000000    0.000000     0.000000  0.001361     18.0   \n",
       "32       0.408108    0.125624     0.192112  0.272481   1202.0   \n",
       "\n",
       "                                         label  \n",
       "0                 Actin filaments (GO:0015629)  \n",
       "1                       Aggresome (GO:0016235)  \n",
       "2                  Cell Junctions (GO:0030054)  \n",
       "3                      Centrosome (GO:0005813)  \n",
       "4                 Cleavage furrow (GO:0032154)  \n",
       "5              Cytokinetic bridge (GO:0045171)  \n",
       "6              Cytoplasmic bodies (GO:0036464)  \n",
       "7                         Cytosol (GO:0005829)  \n",
       "8           Endoplasmic reticulum (GO:0005783)  \n",
       "9                       Endosomes (GO:0005768)  \n",
       "10           Focal adhesion sites (GO:0005925)  \n",
       "11                Golgi apparatus (GO:0005794)  \n",
       "12         Intermediate filaments (GO:0045111)  \n",
       "13                 Lipid droplets (GO:0005811)  \n",
       "14                      Lysosomes (GO:0005764)  \n",
       "15               Microtubule ends (GO:1990752)  \n",
       "16  Microtubule organizing center (GO:0005815)  \n",
       "17                   Microtubules (GO:0015630)  \n",
       "18                        Midbody (GO:0030496)  \n",
       "19                   Midbody ring (GO:0090543)  \n",
       "20                   Mitochondria (GO:0005739)  \n",
       "21                Mitotic spindle (GO:0072686)  \n",
       "22                 Nuclear bodies (GO:0016604)  \n",
       "23               Nuclear membrane (GO:0031965)  \n",
       "24               Nuclear speckles (GO:0016607)  \n",
       "25                       Nucleoli (GO:0005730)  \n",
       "26      Nucleoli fibrillar center (GO:0001650)  \n",
       "27                    Nucleoplasm (GO:0005654)  \n",
       "28                        Nucleus (GO:0005634)  \n",
       "29                    Peroxisomes (GO:0005777)  \n",
       "30                Plasma membrane (GO:0005886)  \n",
       "31                             Rods & Rings ()  \n",
       "32                       Vesicles (GO:0043231)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.13402061855670103\n",
      "0.0\n",
      "nan\n",
      "0.0\n",
      "0.0\n",
      "0.4037735849056604\n",
      "0.1935483870967742\n",
      "nan\n",
      "0.0\n",
      "0.05873715124816446\n",
      "0.041379310344827586\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.12121212121212122\n",
      "0.031088082901554404\n",
      "0.0\n",
      "0.0\n",
      "0.32679738562091504\n",
      "0.0\n",
      "0.1393939393939394\n",
      "0.03314917127071823\n",
      "0.0\n",
      "0.08909730363423213\n",
      "0.0\n",
      "0.4092987804878049\n",
      "0.11727912431587177\n",
      "nan\n",
      "0.17647058823529413\n",
      "0.0\n",
      "0.14380714879467996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "y = torch.cat(y_list).cpu().numpy()\n",
    "y_hat = torch.cat(y_hat_list).cpu().numpy()\n",
    "\n",
    "for i in range(y.shape[1]):\n",
    "    label_inds = np.where(y[:,i]>0)[0]\n",
    "    \n",
    "    print(np.mean(y[label_inds,i] == y_hat[label_inds,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[label_inds,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2161, 32, 23])\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(x.shape)\n",
    "print()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.equal(y.cpu().data.numpy(), nn.Sigmoid()(y_hat).cpu().data.numpy()>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11307868530700314"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.exp(3.219125824868201)/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSLMVVSMACVGFFLLEGPW\n",
      "MMHCTPCLLLMMMMMMMMMM\n"
     ]
    }
   ],
   "source": [
    "enc.train(False)\n",
    "dec.train(False)\n",
    "\n",
    "x_tmp, _ = ds[[np.random.randint(len(ds))]]\n",
    "\n",
    "# x = torch.unsqueeze(x[:,0,:],1)\n",
    "batch_size_tmp = x_tmp.shape[1]\n",
    "\n",
    "hidden = enc.initHidden(batch_size_tmp).cuda(GPU_id)\n",
    "out, hidden = enc(x_tmp, hidden)\n",
    "\n",
    "#input the stop character to the stream    \n",
    "out = Variable(stopChar(batch_size_tmp)).cuda(GPU_id)\n",
    "\n",
    "\n",
    "#     pdb.set_trace()\n",
    "out_chars = list()\n",
    "\n",
    "for i in range(x_tmp.shape[0]):\n",
    "\n",
    "    out, hidden = dec(out, hidden) \n",
    "    \n",
    "    out_chars += [tensorToChar(out)[0,0]]\n",
    "    \n",
    "enc.train(True)\n",
    "dec.train(True)\n",
    "\n",
    "print(''.join(np.hstack(tensorToChar(x_tmp))))\n",
    "print(''.join(out_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008124150603306629"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3E-4*np.log(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.499953655676149e-05"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6.77e-05/np.log(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
