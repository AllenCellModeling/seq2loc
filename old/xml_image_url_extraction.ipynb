{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of scraping xml and saving single channel images + dataframe of metadata and image locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Downloading xmltodict-0.11.0-py2.py3-none-any.whl\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.11.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
      "\u001b[K    100% |################################| 92kB 3.5MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: bs4\n",
      "  Running setup.py bdist_wheel for bs4 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/84/67/d4/9e09d9d5adede2ee1c7b7e8775ba3fbb04d07c4f946f0e4f11\n",
      "Successfully built bs4\n",
      "Installing collected packages: beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.6.0 bs4-0.0.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_single_channel_urls(multi_channel_url, channel_map={'blue':'nucleus', 'red':'microtubules', 'green':'antibody'}):\n",
    "    parname, bname = os.path.split(multi_channel_url)\n",
    "    name, ext = bname.split('.')\n",
    "    plate, well, field = name.split(\"_\")[:3]\n",
    "    channels = name.split(\"_\")[3:]\n",
    "    single_channel_urls = {}\n",
    "    for channel in channels:\n",
    "        sc_name = '_'.join([plate, well, field, channel])\n",
    "        sc_bname = '.'.join([sc_name,ext])\n",
    "        sc_url = os.path.join(parname,sc_bname)\n",
    "        single_channel_urls[channel_map[channel]] = sc_url\n",
    "    seg_name = '_'.join([plate, well, field, 'segmentation'])\n",
    "    seg_bname = '.'.join([seg_name,'png'])\n",
    "    seg_url = os.path.join(parname,seg_bname)\n",
    "    return {'singleChannelUrls':single_channel_urls, 'segmentationUrl':seg_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_xml_tree_from_url(ensembl_input_url):\n",
    "    req = urllib.request.Request(ensembl_input_url)\n",
    "    xml = urllib.request.urlopen(req).read()\n",
    "    return ET.fromstring(xml.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_element(element, tag):\n",
    "    child = [x for x in list(element) if x.tag == tag]\n",
    "    child = child[0] if len(child)==1 else None\n",
    "    return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def ensg_to_antibody_html(ensg_id):\n",
    "    '''Gets the html corresponding to an antibody that targets the ensg_id'''\n",
    "    url = 'https://www.proteinatlas.org/{}/antibody'.format(ensg_id)\n",
    "\n",
    "    req = urllib.request.Request(url)\n",
    "    response = urllib.request.urlopen(req)\n",
    "    html_bin = response.read()\n",
    "    \n",
    "    return html_bin\n",
    "#     soup = BeautifulSoup(html_bin, 'html.parser')\n",
    "\n",
    "def antibody_and_html_to_ensp(antibody_id, antibody_html_bin):\n",
    "    '''Gets the ensp_ids that are targeted by the antibody_id'''\n",
    "    \n",
    "    antibody_html_str = str(antibody_html_bin)\n",
    "    \n",
    "    soup = BeautifulSoup(antibody_html_bin, 'html.parser')\n",
    "    \n",
    "    souplets = soup.find_all('th', attrs={'class':'head last roundtop'})\n",
    "\n",
    "    antibody_ids = list()\n",
    "\n",
    "    for souplet in souplets:\n",
    "        antibody_ids += [re.findall('>Antibody [A-Za-z0-9]*<', str(souplet))[0][10:-1]]\n",
    "\n",
    "    id_index = np.where([id == antibody_id for id in antibody_ids])[0][0]\n",
    "    \n",
    "    start_ind = antibody_html_str.find('Matching transcripts')\n",
    "    end_ind = antibody_html_str.find('<th class=\"sub_head\"', start_ind)\n",
    "    \n",
    "    split = antibody_html_str[start_ind:end_ind].split('<td class=\"\" style=\"\">')[id_index+1]\n",
    "    \n",
    "    ensp_soup = BeautifulSoup(split, 'html.parser')\n",
    "    \n",
    "    souplets = ensp_soup.find_all('a', attrs={'rel': 'nofollow noopener'})\n",
    "    \n",
    "    protein_ids = list()\n",
    "    for souplet in souplets:\n",
    "        protein_ids += [re.findall('ENSP[A-Za-z0-9]* ', str(souplet))[0][:-1]]\n",
    "        \n",
    "    return protein_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_urls_and_info_from_xml(xml_tree):\n",
    "    urls = []\n",
    "    for proteinAtlas in xml_tree.iter('proteinAtlas'):\n",
    "        \n",
    "        identifier = filter_element(filter_element(proteinAtlas, 'entry'), 'identifier')\n",
    "        ensg_id = identifier.attrib['id'] \n",
    "        antibody_html_bin = ensg_to_antibody_html(ensg_id)\n",
    "        \n",
    "        for antibody in proteinAtlas.iter('antibody'):\n",
    "            \n",
    "            antibody_id = antibody.attrib['id']\n",
    "            ensp_ids = antibody_and_html_to_ensp(antibody_id, antibody_html_bin)\n",
    "            \n",
    "            for data in antibody.iter('data'):\n",
    "                for image in data.iter('image'):\n",
    "                    for imageUrl in image.iter('imageUrl'):\n",
    "                        if 'green' in imageUrl.text:\n",
    "                            channels = [x for x in list(image) if x.tag == 'channel']\n",
    "                            channel_info = {channel.attrib['color']:channel.text for channel in channels}\n",
    "                            antigenSequence = filter_element(antibody,'antigenSequence')\n",
    "                            cellLine = filter_element(data, 'cellLine')\n",
    "                            \n",
    "                            xref = filter_element(identifier, 'xref')\n",
    "                            urls += [{'antibody':antibody.attrib['id'] if (antibody is not None and antibody.attrib['id'] is not None) else np.nan,\n",
    "                                      'ENSG':identifier.attrib['id'] if (identifier is not None and identifier.attrib['id'] is not None) else np.nan,\n",
    "                                      'ENSP':ensp_ids,\n",
    "                                      'proteinName':xref.attrib['id'] if (xref is not None and xref.attrib['id'] is not None) else np.nan,\n",
    "                                      'antigenSequence':antigenSequence.text if (antigenSequence is not None and antigenSequence.text is not None) else np.nan,\n",
    "                                      'cellLine':cellLine.text if (cellLine is not None and cellLine.text is not None) else np.nan,\n",
    "                                      'imageUrls':get_single_channel_urls(imageUrl.text, channel_map=channel_info) if (imageUrl is not None and imageUrl.text is not None) else np.nan}]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_local_img_path(url):\n",
    "    return url.split('images/')[-1]\n",
    "\n",
    "def get_img_and_write_local_and_return_path(url, parent_dir):\n",
    "    local_img_path = make_local_img_path(url)\n",
    "    fpath = os.path.join(parent_dir,local_img_path)\n",
    "    if not os.path.isfile(fpath):\n",
    "        req = urllib.request.Request(url)\n",
    "        img = urllib.request.urlopen(req).read()\n",
    "        os.makedirs(os.path.dirname(fpath), exist_ok=True)\n",
    "        with open(fpath, 'wb') as handler:\n",
    "            handler.write(img)\n",
    "    return(local_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensg_to_info(ensg_id):\n",
    "    xml_url = 'https://www.proteinatlas.org/{}.xml'.format(ensg_id)\n",
    "    \n",
    "    doc = make_xml_tree_from_url(xml_url)\n",
    "    urls_and_info = get_urls_and_info_from_xml(doc)\n",
    "        \n",
    "    return urls_and_info \n",
    "#     all_urls_and_info += [*urls_and_info]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6978330cc9ba49798c04cc99565b2e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/opt/conda/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3a2fdb734943d39bf97436100a864a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2672125b0d0f4d65bf3025419c8800e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ensg_ids = ['ENSG00000204209',\n",
    "           'ENSG00000134057',\n",
    "           'ENSG00000126602']\n",
    "\n",
    "save_parent = './data/hpa/{}/'\n",
    "\n",
    "\n",
    "for ensg_id in tqdm_notebook(ensg_ids):\n",
    "    \n",
    "    \n",
    "    save_dir = save_parent.format(ensg_id)\n",
    "    save_file = save_dir + '/info.csv'\n",
    "    \n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    urls_and_info = ensg_to_info(ensg_id)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['ENSG',  'ENSP', 'proteinName', 'antibodyName', 'antigenSequence', 'cellLine',\n",
    "                           'antibodyChannel', 'microtubuleChannel', 'nuclearChannel', 'segmentationChannel'])\n",
    "    \n",
    "    for u in tqdm_notebook(urls_and_info):\n",
    "        \n",
    "        antibody_path_local = get_img_and_write_local_and_return_path(u['imageUrls']['singleChannelUrls']['antibody'], save_dir)\n",
    "        microtubule_path_local = get_img_and_write_local_and_return_path(u['imageUrls']['singleChannelUrls']['microtubules'], save_dir)\n",
    "        nuclear_path_local = get_img_and_write_local_and_return_path(u['imageUrls']['singleChannelUrls']['nucleus'], save_dir)\n",
    "        segmentation_path_local = get_img_and_write_local_and_return_path(u['imageUrls']['segmentationUrl'], save_dir)\n",
    "\n",
    "        df = df.append({'ENSG':u['ENSG'],\n",
    "                        'ENSP':u['ENSP'],\n",
    "                        'proteinName':u['proteinName'],\n",
    "                        'antibodyName':u['antibody'],\n",
    "                        'antigenSequence':u['antigenSequence'],\n",
    "                        'cellLine':u['cellLine'],                    \n",
    "                        'antibodyChannel':antibody_path_local,\n",
    "                        'microtubuleChannel':microtubule_path_local,\n",
    "                        'nuclearChannel':nuclear_path_local,\n",
    "                        'segmentationChannel':segmentation_path_local}, ignore_index=True)\n",
    "\n",
    "    df.to_csv(save_file)\n",
    "    \n",
    "# for xml_url in tqdm_notebook(xml_urls):\n",
    "#     doc = make_xml_tree_from_url(xml_url)\n",
    "#     urls_and_info = get_urls_and_info_from_xml(doc)\n",
    "#     all_urls_and_info += [*urls_and_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
