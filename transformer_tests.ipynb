{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages (from -r requirements.txt (line 1))\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages (from -r requirements.txt (line 2))\n",
      "Requirement already satisfied: torchtext>=0.2.1 in /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages (from -r requirements.txt (line 3))\n",
      "Requirement already satisfied: future in /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages (from -r requirements.txt (line 4))\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages (from torchtext>=0.2.1->-r requirements.txt (line 3))\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages (from requests->torchtext>=0.2.1->-r requirements.txt (line 3))\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages (from requests->torchtext>=0.2.1->-r requirements.txt (line 3))\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages (from requests->torchtext>=0.2.1->-r requirements.txt (line 3))\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages (from requests->torchtext>=0.2.1->-r requirements.txt (line 3))\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Obtaining file:///root/projects/OpenNMT-py\n",
      "Installing collected packages: OpenNMT-py\n",
      "  Found existing installation: OpenNMT-py 0.1\n",
      "    Uninstalling OpenNMT-py-0.1:\n",
      "      Successfully uninstalled OpenNMT-py-0.1\n",
      "  Running setup.py develop for OpenNMT-py\n",
      "Successfully installed OpenNMT-py\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../OpenNMT-py; pip install -r requirements.txt; pip install -e ./\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(10, 512, padding_idx=9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import onmt\n",
    "\n",
    "\n",
    "embedding = onmt.modules.Embeddings(word_vec_size = 512,\n",
    "                 word_vocab_size = 10,\n",
    "                 word_padding_idx = 9,\n",
    "                 position_encoding=True)\n",
    "\n",
    "for e in embedding.make_embedding.emb_luts:\n",
    "    print(e)\n",
    "# seq = Variable(torch.Tensor([[[0,0,0,0,0,0,0,0,0,1], [0,0,0,0,0,0,0,0,0,1]]]))\n",
    "\n",
    "# embedding(Variable(torch.Tensor([[[1],[2], [9]], [[1], [2], [9]]]).long()))\n",
    "\n",
    "# embedding(seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.modules import TransformerEncoder, TransformerDecoder\n",
    "\n",
    "\n",
    "# enc(torch.Tensor)\n",
    "enc = TransformerEncoder(num_layers = 10, hidden_size = 512, dropout = 0.2, embeddings = embedding)\n",
    "dec = TransformerDecoder(num_layers = 10, hidden_size = 512, attn_type = 'dot', copy_attn = False, dropout = 0.2, embeddings = embedding)\n",
    "\n",
    "model = onmt.Models.NMTModel(enc, dec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n",
      "torch.Size([3, 3, 1])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "seq_in = Variable(torch.Tensor([[[1],[2], [9]], [[1], [2], [9]]]).long())\n",
    "seq_out = Variable(torch.Tensor([[[1],[2], [9]], [[1], [2], [9]], [[1], [2], [9]]]).long())\n",
    "# seq_out = Variable(torch.Tensor([[1,2,9], [1,2,9]]).long())\n",
    "\n",
    "seq_lens = Variable(torch.Tensor([2,1,0]).long())\n",
    "\n",
    "print(seq_in.shape)\n",
    "print(seq_out.shape)\n",
    "print(seq_lens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.1984  1.1599  0.7904  ...  -0.6174 -0.4355  0.3219\n",
      " -0.8556  0.2818 -0.5889  ...   0.2970  1.4937  0.3822\n",
      "  1.2172  1.1395 -0.5514  ...   0.7319 -2.0153  1.5844\n",
      "\n",
      "( 1 ,.,.) = \n",
      " -0.2069  1.1144  0.9068  ...  -0.5726 -0.4102  0.4313\n",
      " -1.0314  0.2655 -0.5982  ...   0.3396  1.4709  0.3343\n",
      "  1.5037  0.6424 -0.2574  ...   0.6737  0.1966  0.5882\n",
      "[torch.FloatTensor of size (2,3,512)]\n",
      "\n",
      "{'std': Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.5027  0.4973\n",
      "  0.4959  0.5041\n",
      "  0.5000  0.5000\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.5031  0.4969\n",
      "  0.4958  0.5042\n",
      "  0.5000  0.5000\n",
      "[torch.FloatTensor of size (2,3,2)]\n",
      "}\n",
      "<onmt.modules.Transformer.TransformerDecoderState object at 0x7f7a2e5c8a58>\n"
     ]
    }
   ],
   "source": [
    "#for whatever reason in the code the last element of the target sequence is removed\n",
    "\n",
    "a, b, c = model(seq_in, seq_out, seq_lens)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///root/projects/attention-is-all-you-need-pytorch\n",
      "Installing collected packages: transformer\n",
      "  Found existing installation: transformer 0.1.0\n",
      "    Uninstalling transformer-0.1.0:\n",
      "      Successfully uninstalled transformer-0.1.0\n",
      "  Running setup.py develop for transformer\n",
      "Successfully installed transformer\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../attention-is-all-you-need-pytorch; pip install -e ./\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer\n",
    "from transformer.Layers import EncoderLayer\n",
    "from transformer.Models import get_attn_padding_mask\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "import seq2loc\n",
    "import seq2loc.models\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enc = EncoderLayer(24, 256, 6, 64, 64).cuda(0)\n",
    "\n",
    "# enc = transformer.Models.EncoderSimple(n_max_seq = 2000, n_layers=1, n_head=8, d_k=128, d_v=128,\n",
    "#             d_word_vec=24, d_model=24, d_inner_hid=1024, dropout=0.1).cuda(0)\n",
    "\n",
    "    \n",
    "model = seq2loc.models.TransformerClassifier(24, 2, growth_rate = 64, max_seq_len = 2000).cuda(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr = 1E-3, betas=(0.5, 0.999))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "x = Variable(torch.rand(batch_size, 24, 128)).cuda(0)\n",
    "x[:,:,-1] = 0\n",
    "\n",
    "output = model(x)\n",
    "\n",
    "loss = criterion(output, Variable(torch.zeros(list(output.shape)).float().cuda(0)))\n",
    "loss.backward()\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 24])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.FloatTensor' object has no attribute 'get_device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-58d9186e7d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.FloatTensor' object has no attribute 'get_device'"
     ]
    }
   ],
   "source": [
    "torch.Tensor([1,2,3]).get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf\n"
     ]
    }
   ],
   "source": [
    "if hasattr(torch.Tensor([1,2,3]).cuda(0), 'get_device'): print('asdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
